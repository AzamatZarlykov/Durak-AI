\chapter{Game and AI Implementation}

The primary objective of this study is to design and develop a framework for the implementation and evaluation of artificial intelligence (AI) agents in Durak. Since the framework in this game is tailored to support the application and evaluation of AI algorithms within the game environment, the focus of this chapter is to provide a high-level overview of the framework's architecture, including its capabilities for supporting the development and evaluation of AI agents for Durak.

\section{OS support}

I tested the game framework for Durak on both the Windows 10 and Linux operating systems to confirm compatibility and functionality. While this list is not exhaustive, and the framework may potentially run on other operating systems, these are the only ones that were formally tested.

\section{A High-Level View of the Framework}

The Durak AI framework includes a game model, AI agents, and a command-line interface (CLI) that are implemented using the C\# programming language and targeted for the .NET 6 platform. C\# was selected as the programming language for the implementation of aforementioned projects in Durak. The language offers benefits from a highly optimized Just-In-Time (JIT) compiler, resulting in enhanced speed. This attribute made C\# an ideal choice for this development.

In order to run and test the program, the project has to be cloned from the repository . The source code for the project is available on \underline{\href{https://github.com/AzamatZarlykov/Durak-AI}{GitHub}}. From the CLI directory of the project, the user can use the command line to enter the following command: 
\begin{lstlisting}
$ dotnet run
\end{lstlisting}
This will launch the command-line interface and provide guidance on how to proceed with experimentation (for additional information, please refer to Section \ref{CLI}).

An analysis of the source code using Visual Studio's Calculate Code Metrics for Solution feature revealed that the solution consists of a total of 3034 lines of source code. This includes 993 lines in the Agent project, 757 lines in the CLI project, and 1284 lines in the game model.

\subsection{Project Structure}
The game Durak is organized within a solution file, with the file extension ``.sln'', which is a type of file used to manage projects in Visual Studio. This solution includes three individual projects: 

\begin{itemize}

\item Model - A C\# library project that contains the game logic for Durak.

\item Agent - A C\# library that contains all of the implemented AI agents.

\item CLI - A C\# Command-Line Interface (CLI) project that includes parameters for modifying the game model and agents settings in order to perform experiments.

The aforementioned components will be further discussed in the following subsections.

\end{itemize}

\subsection{Model}

The game model, which represents the current state of the game, is implemented using object-oriented programming principles. As it was mentioned before, the game logic for Durak is contained within the \textbf{Model} C\# library, which serves as a modular and reusable unit. It includes class objects, such as Player, Card, and Deck, as well as all of the other main components that make up the game each of which is equipped with the necessary methods to support the game's functionality. These objects and their methods are designed to reflect the key components and features described in the game description.

\begin{figure}[h]
    \centering
    \captionsetup{justification=centering}
    \includegraphics[width=0.9\textwidth]{../img/modelUML.png}
    \caption{A simplified UML diagram showing the relationship between the objects within the Model library.}
    \label{fig:modelUML}
\end{figure}

Before delving into the description of the game state and components of Durak, it is useful to first consider the relationship of all of the objects in the model to that state. A class diagram illustrating the relationships between the objects in the model can be found in Figure \ref{fig:modelUML}.

The representation of the game state \texttt{Durak} in the model is a key aspect of the overall system. This representation holds all of the necessary information and logic required to play the game of Durak, shown in Figure \ref{fig:codeDurak}, and therefore plays a central role in the functioning of the model. As such, it is important to carefully consider the design and implementation of the game state representation along with its components. 

\begin{figure}[h]
\captionsetup{justification=centering}
\begin{lstlisting}
// Bout object of the game
private Bout bout;

// Deck object of the game
private Deck deck;

// Trump card of the game that can be assigned or not
private Card? trumpCard;

// Representation of the discard pile in the game
private List<Card> discardPile = new List<Card>();

// Players inside the game
private List<Player> players = new List<Player>();

\end{lstlisting}
\caption{A simplified diagram of the Durak class, which encompasses the main properties of the game}
\label{fig:codeDurak}
\end{figure}

The object in question serves as a comprehensive representation of all game states and data throughout a single game of Durak. Because of that it is utilized to communicate this information to other components within the system, such as the command-line interface (CLI) or artificial intelligence (AI) scripts. To facilitate communication and coordination between the Durak model and the agents that interact with it, the game state provides two primary functions: \texttt{PossibleMoves}, shown in Figure \ref{fig:codePossibleMoves} and \texttt{Move}, shown in Figure \ref{fig:codeMove}. These functions serve as the primary means through which changes can be made to the game state, and as such, play a crucial role in the overall operation of the model. 

\begin{figure}[h]
\captionsetup{justification=centering}
\begin{lstlisting}

if (turn == Turn.Attacking){
    	if (CanAttack() && OpponentCanFitMoreCards()) {
		return GenerateListOfAttackingCards();
	} else {
		// passing the attack
		return null;	
	}
}
else {
	Card attackingCard = bout.GetAttackingCards()[^1]
	if (CanDefend(attackingCard)) {
		return GenerateListofDefendingCards(attackingCard);
	} else {
		// taking the cards
		return null
	}
}
\end{lstlisting}
\caption{A simplified overview of the PossibleMoves method inside the Durak class}
\label{fig:codePossibleMoves}
\end{figure}

The \texttt{PossibleMoves} method determines the list of actions that are available to the current player based on the current game state and the rules of the game. When it is the attacker's turn, the method considers the rules for attacking (details in section \ref{attackconditions}) and generates a list of eligible cards that can be played or allows the player to pass if no suitable cards are available. Similarly, when it is the defender's turn (details in section \ref{BeatingRule}), the method takes into account the card being attacked and generates a list of cards that can be played to defend or offers the option to take the attack if no suitable defense is available. This enables the method to adapt to the specific circumstances of the game and provide appropriate options for the current player to make a move.

The \texttt{Move} method modifies the current game state by executing the action chosen by the current player. This move is selected by the agent, which performs calculations based on the possible moves generated by the \texttt{PossibleMoves} method. The specific nature of these calculations depends on the type of agent being used. For example, a rule-based agent may simply select the lowest value rank card, while a more sophisticated agent, such Monte-Carlo Tree Search (MCTS), may use more complex decision-making processes to determine the optimal move to make. Regardless of the type of agent being used, the \texttt{Move} method ultimately updates the game state to reflect the chosen action and advances the game to the next turn.

\begin{figure}[h]
\captionsetup{justification=centering}
\begin{lstlisting}
if (turn == Turn.Attacking){
	// the attacker played a card	
	if (card is not null) {
		attacker.GetHand().Remove(card);
		bout.AddCard(card);
	} else {
		bout.RemoveCards();
		return;
	}
}
else {
	// the defender beat the attacking card
	if (card is not null){
		defender.GetHand().Remove(card);
		bout.AddCard(card);
	} else {
		FillPlayerHand(bout.GetEverything(), defender)
		return;
	}	
}
turn = turn == Turn.Attacking ? Turn.Defending : Turn.Attacking;
\end{lstlisting}
\caption{A simplified overview of the Move method inside the Durak class}
\label{fig:codeMove}
\end{figure}

Additionally, it is important to note that, for security purposes, the agents are not provided with the entire \texttt{Durak} object. Instead, they are given access to a \texttt{GameView} class representation, which allows them to obtain essential information about the current game state and make changes through the methods outlined in Figures \ref{fig:codePossibleMoves} and \ref{fig:codeMove}. This approach ensures that the agents are not able to manipulate the game in an unauthorized manner.


\subsection{CLI}
\label{CLI}

The command-line interface (CLI) plays a crucial role in the architecture of the application. Through the CLI, the user can interact with the application using a text-based interface, providing parameters and receiving feedback or results. The CLI enables a range of experimental and testing scenarios, including the ability to conduct playouts between different agents within a customizable game environment that can be modified by altering various parameters. The flexibility and versatility of the CLI is crucial for exploring the capabilities of the system and evaluating the performance of the AI agents. In this chapter, we will examine the various parameters that can be used to manipulate the behavior of the agents and the game environment through the CLI. This will provide insight into the capabilities of the system and enable a more thorough evaluation of the AI agents' performance.

Before discussing the organizational structure of the project, it is important to introduce the parameters and their roles within the project (Please refer to Figure \ref{fig:parameters}). This will facilitate a better understanding of the various components and how they interact with one another. 

\begin{table}
\captionsetup{justification=centering}
\begin{center}
\resizebox{\textwidth}{!}{%
\begin{tabular}{ | m{0.2\textwidth} | m{0.8\textwidth}| } 
  \hline
   Parameter & Description \\
  \hline
  -ai1 & The agent for player 1. (String)(Default = random) \\ 
  \hline
  -ai2 & The agent for player 2. (String)(Default = random) \\ 
  \hline
  -config & Used for grid search parameter configuration.(Default = False) \\ 
  \hline
  -d1 & Displays \# of states \& depth for minimax move(Default = False) \\ 
  \hline
  -d2 & Displays all the moves that minimax considers(Default = False) \\ 
  \hline
  -include\_trumps & Enable trump cards in the game(Default = True) \\ 
  \hline
  -log & Enable logs for writing in the file(Default = False) \\ 
  \hline
  -open\_world & Make all cards visible to both players(Default = False) \\ 
  \hline
  -seed & A seed for random number generation(Int32) \\ 
  \hline
  -start\_rank & The starting rank of cards in the deck(Int32)(Default = 6) \\ 
  \hline
  -total\_games & The number of games to play(Int32)(Default = 1000) \\ 
  \hline
  -tournament & Runs the tournament with the agents specified. \\ 
  \hline
  -verbose & Enable verbose output(Default = False) \\ 
  \hline
  
\end{tabular}}
\end{center}
\caption{\label{paramsTable} A text-based 'Help' statement that describes the parameters that can be used}
\end{table}

There are various ways to utilize the aforementioned parameters. An example of using the default settings to run a game between RandomAI agent(random) and GreedyAI agent(greedy) is provided below. Note that the user can select different agents as well:

\begin{lstlisting}
$ dotnet run --project CLI -ai1=random -ai2=greedy
\end{lstlisting}

This command initiates the simulation of 1000 games between the RandomAI and GreedyAI agents in a fully enclosed environment (with a starting rank of 6) and provides the following output to the console:

\begin{lstlisting}
==== RUNNING ====

Game 1: Agent 1 (random) won. Total bouts: 21
Game 2: Agent 2 (greedy) won. Total bouts: 19
Game 3: Agent 2 (greedy) won. Total bouts: 13
Game 4: Agent 2 (greedy) won. Total bouts: 18
Game 5: Agent 2 (greedy) won. Total bouts: 18
...
\end{lstlisting}

To more thoroughly analyze the results of the specific game, \texttt{seed} with the game id and the \texttt{-verbose}  parameters may be utilized. This provides detailed information about the progression of the game by showing every possible move, the chosen move and other game related details. An example of the first game in which a RandomAI agent defeats a GreedyAI agent using this parameter is shown below:


\begin{lstlisting}
$ dotnet run --project CLI -ai1=random -ai2=greedy -verbose -seed=1
\end{lstlisting}

The command above generates a verbose output, as shown below. It should be noted that this is only a portion of the full output and the blue colored suits are the indications of the trump suit.

\begin{lstlisting}
==== START ====

Trump card: A(*@$\textcolor{blue}{\diamondsuit}$@*)
Deck's size: 36

Player 1 (random) cards: 9(*@$\textcolor{black}{\spadesuit}$@*)  A(*@$\textcolor{black}{\spadesuit}$ @*) 10(*@$\textcolor{red}{\heartsuit}$@*)  Q(*@$\textcolor{red}{\heartsuit}$@*)  9(*@$\textcolor{blue}{\diamondsuit}$@*)  K(*@$\textcolor{blue}{\diamondsuit}$@*)
Player 2 (greedy) cards: 6(*@$\textcolor{black}{\spadesuit}$@*)  8(*@$\textcolor{black}{\spadesuit}$@*)  Q(*@$\textcolor{black}{\spadesuit}$@*)  7(*@$\textcolor{red}{\heartsuit}$@*)  A(*@$\textcolor{red}{\heartsuit}$@*)  6(*@$\textcolor{black}{\clubsuit}$@*)

=== New Bout ===

TURN: Player 1 (random) (Attacking)
Can attack
Possible cards: 9(*@$\textcolor{black}{\spadesuit}$@*)  A(*@$\textcolor{black}{\spadesuit}$@*)  10(*@$\textcolor{red}{\heartsuit}$@*)  Q(*@$\textcolor{red}{\heartsuit}$@*)  9(*@$\textcolor{blue}{\diamondsuit}$@*)  K(*@$\textcolor{blue}{\diamondsuit}$@*)
Attacks: 9(*@$\textcolor{black}{\spadesuit}$@*)

Bout 1:
Attacking cards: 9(*@$\textcolor{black}{\spadesuit}$@*)
Defending cards:

TURN: Player 2 (greedy) (Defending)
Can defend
Possible cards: Q(*@$\textcolor{black}{\spadesuit}$@*)
Defends: Q(*@$\textcolor{black}{\spadesuit}$@*)

Bout 1:
Attacking cards: 9(*@$\textcolor{black}{\spadesuit}$@*)
Defending cards: Q(*@$\textcolor{black}{\spadesuit}$@*)
...
\end{lstlisting}

Upon completion of an experiment, the program presents statistical analysis on all simulations conducted using the specified game and agent configuration. This analysis includes various metrics, such as the average number of rounds played per game, the average number of moves made per bout, the average time taken per move by each agent, the win rate, and the \textbf{Wilson confidence interval} between the two agents.

\begin{figure}[h]
\captionsetup{justification=centering}
\begin{lstlisting}
$ dotnet run --project CLI -ai1=greedy -ai2=random -open_world -total_games=1000 
			-start_rank=6 -include trumps
...

==== STATISTICS ====
Total games played: 1000

Average bouts played over the game: 17.1
Average moves per bout over the game: 3.1

Average time per move (greedy agent): 0.0201ms
Average time per move (random agent): 0.0181ms

Draw rate: 0.8%
Agent 1 (greedy) won 913 / 1000 games (91.3%)
Agent 2 (random) won 79 / 1000 games (7.9%)

With 98% confidence, Agent 1 (greedy) wins between 89.8% and 93.8% 
With 98% confidence, Agent 2 (random) wins between 6.2% and 10.2% 
\end{lstlisting}
\caption{A representation of command and statistics.}
\label{fig:statistics}
\end{figure}

I chose Wilson's confidence interval in order to assess the statistical significance of the observed difference in win rates between the two players. By using this measure, it was possible to determine whether the observed difference in win rates was likely to be a true reflection of the relative abilities of the players, or whether it may have occurred by chance. Overall, the statistical data generated from the simulations was useful for identifying potential errors and for optimizing the strategies for the rule-based agents. As an example, the figure \ref{fig:statistics}  presents the statistical result of a simulation comparing the performance of greedy and random agents in a full open-world game.

Other than that, it is important to consider that when utilizing certain advanced agents, such as Monte-Carlo Tree Search (MCTS) and Minimax, it is necessary to specify their respective parameters in order to effectively utilize their capabilities. 

In the context of the Minimax algorithm, it is necessary to specify the value of the \texttt{depth} parameter, which determines the depth to which the search tree will be explored in order to identify the optimal move. In addition to the depth parameter, it is also necessary to specify the \texttt{eval} parameter, which specifies the heuristic function used to evaluate the state when the maximum depth has been reached. The \texttt{eval} parameter can take on either the value \texttt{basic} or \texttt{playout} (additional information on these parameters can be found in Section \ref{minimax}). It is worth noting that, in a closed world scenario, the \texttt{samples} parameter is optional and has a default value of 20. The \texttt{samples} parameter serves to prevent agents such as Minimax and MCTS from cheating by accessing information about future states of the game (details in Section \ref{AIAgents}). This illustration presents an example of a simulation between the Minimax and greedy agents with arbitrary parameter values in the open and closed world:\\

\textbf{Open-world: }
\begin{lstlisting}
dotnet run --project CLI -ai1=minimax:depth=4,eval=basic -ai2=greedy -open_world
\end{lstlisting}

\textbf{Closed-world: }
\begin{lstlisting}
dotnet run --project CLI -ai1=minimax:depth=6,eval=basic,samples=15 -ai2=smart
\end{lstlisting}

Regarding MCTS, it is necessary to specify the value of the \texttt{limit} parameter, which determines the computational budget allocated for the algorithm to build the search tree. The search is halted and the best-performing root action is returned once this budget is reached. The \texttt{c}, which is called exploration constant, parameter also needs to be specified because it determines the balance between exploitation and exploration in the UCB equation (detailed information is provided in Section \ref{MCTS}). This parameter is set to a default value of  $\sqrt{2}$ ($\approx 1.41$) and can be adjusted to fine-tune the performance of the algorithm. The \texttt{simulation} parameter should also be specified, indicating whether to use a smart simulation (\texttt{greedy}) or a random simulation (\texttt{random}). Lastly, in a closed world setting, just like in Minimax, it is necessary to specify the \texttt{samples} parameter to prevent the MCTS agent from cheating by considering future states of the game. This example demonstrates a simulation between MCTS and greedy agents with arbitrary parameter values in open and closed world settings: \\

\textbf{Open-world: }
\begin{lstlisting}
dotnet run --project CLI -ai1=mcts:limit=100,simulation=greedy -ai2=greedy 
							-open_world
\end{lstlisting}

\textbf{Closed-world: }
\begin{lstlisting}
dotnet run --project CLI -ai1=mcts:limit=100,simulation=greedy,samples=10 
							-ai2=greedy
\end{lstlisting}



\subsection{Agent}

The Agent library is a collection of artificial intelligence (AI) agents that are utilized in the experiment. The agents included in this library are: Random, Greedy, Smart, Minimax, and MCTS. The functionality and implementation of these agents will be elaborated upon in the subsequent chapter. In this section, focus on the overall structure and organization of the Agent library is placed.

The abstract class \texttt{Agent} represents a common interface for all agents, enabling them to make a move based on a given set of options. This is achieved through the implementation of a abstract method called \texttt{Move}. A visual representation of the abstract class \texttt{Agent} is provided in Figure \ref{fig:abstractClass}.

\begin{figure}[h]
\captionsetup{justification=centering}
\begin{lstlisting}
public abstract class Agent
{
    	public string ?name;
	public string? GetName() => name;
    	public abstract Card? Move(GameView gameView);
}
\end{lstlisting}
\caption{A representation of the abstract class 'Agent'}
\label{fig:abstractClass}
\end{figure}

As an example, we can examine the behavior of a Random agent, which selects a move from the list of possible moves at random, can be examined. This process is demonstrated in Figure \ref{fig:randomMove}. The Move method of the Random agent is responsible for implementing this behavior. The figure illustrates the process of overriding the abstract Move method in order to modify the behavior of the function based on the random nature. This process is followed for all agents that must implement this method, resulting in diverse behaviors depending on the agent in question.